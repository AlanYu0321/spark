{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Removing hard coded password - using os module & open to import them from creds.txt file\n",
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    creds_file = (\n",
    "        (open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\")).read().strip().split(\",\")\n",
    "    )\n",
    "    accesskey, secretkey = creds_file[0], creds_file[1]\n",
    "except:\n",
    "    print(\"File not found, you can't access minio\")\n",
    "    accesskey, secretkey = \"\", \"\"\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.3\")\n",
    "conf.set(\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "    \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n",
    ")\n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", accesskey)\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", secretkey)\n",
    "# Configure these settings\n",
    "# https://medium.com/@dineshvarma.guduru/reading-and-writing-data-from-to-minio-using-spark-8371aefa96d2\n",
    "conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# https://github.com/minio/training/blob/main/spark/taxi-data-writes.py\n",
    "# https://spot.io/blog/improve-apache-spark-performance-with-the-s3-magic-committer/\n",
    "conf.set(\"spark.hadoop.fs.s3a.committer.magic.enabled\", \"true\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.committer.name\", \"magic\")\n",
    "# Internal IP for S3 cluster proxy\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\")\n",
    "# Send jobs to the Spark Cluster\n",
    "conf.setMaster(\"spark://sm.service.consul:7077\")\n",
    "# Set driver and executor memory\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"THY\")\n",
    "    .config(\"spark.driver.host\", \"spark-edge.service.consul\")\n",
    "    .config(conf=conf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Logs\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# Reading From MinIO\n",
    "df = spark.read.csv(\"s3a://itmd521/50.txt\")\n",
    "\n",
    "splitDF = (\n",
    "    df.withColumn(\"WeatherStation\", df[\"_c0\"].substr(5, 6))\n",
    "    .withColumn(\"WBAN\", df[\"_c0\"].substr(11, 5))\n",
    "    .withColumn(\"ObservationDate\", to_date(df[\"_c0\"].substr(16, 8), \"yyyyMMdd\"))\n",
    "    .withColumn(\"ObservationHour\", df[\"_c0\"].substr(24, 4).cast(IntegerType()))\n",
    "    .withColumn(\"Latitude\", df[\"_c0\"].substr(29, 6).cast(\"float\") / 1000)\n",
    "    .withColumn(\"Longitude\", df[\"_c0\"].substr(35, 7).cast(\"float\") / 1000)\n",
    "    .withColumn(\"Elevation\", df[\"_c0\"].substr(47, 5).cast(IntegerType()))\n",
    "    .withColumn(\"WindDirection\", df[\"_c0\"].substr(61, 3).cast(IntegerType()))\n",
    "    .withColumn(\"WDQualityCode\", df[\"_c0\"].substr(64, 1).cast(IntegerType()))\n",
    "    .withColumn(\"SkyCeilingHeight\", df[\"_c0\"].substr(71, 5).cast(IntegerType()))\n",
    "    .withColumn(\"SCQualityCode\", df[\"_c0\"].substr(76, 1).cast(IntegerType()))\n",
    "    .withColumn(\"VisibilityDistance\", df[\"_c0\"].substr(79, 6).cast(IntegerType()))\n",
    "    .withColumn(\"VDQualityCode\", df[\"_c0\"].substr(86, 1).cast(IntegerType()))\n",
    "    .withColumn(\"AirTemperature\", df[\"_c0\"].substr(88, 5).cast(\"float\") / 10)\n",
    "    .withColumn(\"ATQualityCode\", df[\"_c0\"].substr(93, 1).cast(IntegerType()))\n",
    "    .withColumn(\"DewPoint\", df[\"_c0\"].substr(94, 5).cast(\"float\"))\n",
    "    .withColumn(\"DPQualityCode\", df[\"_c0\"].substr(99, 1).cast(IntegerType()))\n",
    "    .withColumn(\"AtmosphericPressure\", df[\"_c0\"].substr(100, 5).cast(\"float\") / 10)\n",
    "    .withColumn(\"APQualityCode\", df[\"_c0\"].substr(105, 1).cast(IntegerType()))\n",
    "    .drop(\"_c0\")\n",
    ")\n",
    "\n",
    "splitDF.printSchema()\n",
    "splitDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitDF.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "    \"s3a://tyu17/50.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture thyapp\n",
    "# average temperature and standard deviation per month, per year.\n",
    "avg_df = (\n",
    "    splitDF.select(\n",
    "        month(col(\"ObservationDate\")).alias(\"Month\"),\n",
    "        year(col(\"ObservationDate\")).alias(\"Year\"),\n",
    "        col(\"AirTemperature\").alias(\"Temperature\"),\n",
    "    )\n",
    "    .groupBy(\"Month\", \"Year\")\n",
    "    .agg(avg(\"Temperature\"), stddev(\"Temperature\"))\n",
    "    .orderBy(\"Year\", \"Month\")\n",
    ")\n",
    "\n",
    "avg_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop your session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
